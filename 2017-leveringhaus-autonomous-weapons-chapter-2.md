% Leveringhaus: Ethics and Autonomous Weapons - Chapter 2
% Alexandra Haas | Michael Czechowski
% Seminar: Prof. Dr. Catrin Misselhorn - Kriegsroboter, Drohnen und Co. - Zur Ethik autonomer Waffensysteme (WiSe 2017/18)
---
  papersize: a4paper
  geometry: left=4cm
  geometry: top=2.5cm
  fontsize: 12pt
  toc: no
  lang: de
...

# Wichtigkeit des Designs

## Was ist eine Waffe?

- Der Unterschied zwischen Waffen und anderen Artefakten liegt in Ihrem jeweiligen Design und dessen darin liegende Intention. 

- Im Design einer Waffe liegt die Intention Schaden oder Verletzungen anzurichten. 
  *Eine nicht verletzende Waffe ist ein Wiederspruch in sich.*

## Problem der militärischen „Support-Systems“ und die Möglichkeit moralischer Rechtfertigung in Bezug auf die "Doctrine of Double-Effect":

- Die Doktrin besagt, dass es konzeptuell und normativ notwendig ist, zwischen dem zu unterscheiden, was ein Akteur beabsichtigt und was ein Akteur vorhersieht.

- Annahme der Waffenbefürworter: Die primäre Intention von Radarsystemen liegt nicht darin Schaden anzurichten, kann aber durch einen Nebeneffekt zu Schaden führen. Bsp.: Man wollte das Land beschützen und konnte nicht wirklich vorhersehen, dass dabei Unmengen von Zivilisten getötet wurden. 

## Leveringhaus' Gegenargument:

Sein Gegenargument basiert auf der richtigen Interpretation von der Doktrin und der damit im Zusammenhang stehenden primären Komponente beim Design einer Waffe, nämlich, dass sie fähig sein muss Schaden anzurichten. 

# Verletzung und Schaden als zetrales Element

## Leveringhaus' Definition von Schädigung/Verletzung richtet sich nach Joel Feinberg:

Schaden oder Verletzung im unmoralischen Sinn wird über zwei Komponenten definiert: 

(1) Meine schädigende/verletzende Handlung durch eine Waffe bedeutet einen Rückschlag der Interessen einer anderen Person.

(2) Meine schädigende/verletzende Handlung durch eine Waffe zieht eine Rechtsverletzung der geschädigten Person nach sich. (Bsp.: Recht auf Privatsphäre)

## Unterscheidung zwischen dreierlei Arten von Waffen:

(1) Waffen die keine tödlichen oder damit in Beziehung stehenden Verletzungen anrichten: Bsp. Eine Computersoftware richtet zwar einen Schaden an, dieser ist aber nicht tödlich und führt auch zu einem späteren Zeitpunkt nicht zum Tod.

(2) Waffen, die tödlichen oder damit in Beziehung stehende Verletzungen als Nebeneffekt anrichten. Bsp.: Truppe A blockiert das Radarsystem von Truppe B.

(3) Waffen, deren primäre Funktion es ist, tödlichen oder einen damit in Beziehung stehenden Schaden anzurichten. Bsp.: Maschinengewehr.

**Der 2. & 3. Punkt sind wesentlich für die Diskussion um autonome Waffen.**

## Leveringhaus' Antwort auf Einwände:

### 1. Einwand: 

Bezüglich der Verwendung von Waffen ist es fraglich, ob das Zerstören einer beispielsweise Rakete einen Rückschlag der Interessen einer Person darstellt oder gar die Verletzung der Rechte von Personen. 

**Leveringhaus‘ Antwort**

Wenn „nur“ militärische Artefakte, wie etwa Raketen oder Tanklaster beschädigt werden, kann man trotzdem von einem Rückschlag der Interessen von Menschen sprechen, weil diese als Steuerzahler die militärischen Artefakte finanzieren. Der Schaden an den Rechten einer Person bleibt fraglich.

### 2. Einwand:

Unterscheidung von offensiven und defensiven Waffen fordert eine Unterscheidung zwischen offensiver und defensiver Schädigung oder Verletzung in Bezug auf die moralische Rechtfertigung.

**Leveringhaus' Antwort**

Für ethische Fragen ist mit dieser Unterscheidung nichts gewonnen. Auch defensive Waffen sind moralisch problematisch, weil sie a. die Fähigkeit der Gegner unterminieren sich verteidigen zu können und b. missbraucht werden können um einen ungerechten Krieg zu initiieren. 

# Autonomie einer Maschine und Waffentechnolgie

## Inhaltlicher Aufbau

- Was ist Autonomie? (ab S. 46)
- Was ist Unabhängigkeit? (ab S. 48)
- Unterscheidung zwischen Drohnen und *unmanned weapons*? (ab S. 49)
- Was sind *kognitive Systeme* und *künstliche Akteure*? (ab S. 50)
- Zielerfassung (ab S. 53)
  - Das *Generating Model* (ab S. 53 unten)
  - Das *Execution Model* (ab S. 56)
- Zusammenfassung (ab S. 57) 

## Argumente

### Autonomie von Maschinen

(1) Wenn eine Maschine das Design bzw. den Zweck hat Verletzungen und Schaden zuzufügen, handelt es sich um eine Waffe.

(2) Die Entwicklung von Waffen steht unter bestimmten rechtlichen sowie moralischen Regulierungen.

(3) Sog. autonome Waffen werden zwecks der Verursachung von Verletzungen und Schaden entwickelt und sollten somit unter der Berücksichtigung rechtlicher sowie moralischer Regulierungen entwickelt werden. [1, 2]

(4) Es wird angenommen, dass alle vorherschenden Waffensysteme in Benutzung unter einer moralischen und rechtlichen Regulierung stehen.

(5) Wenn sog. autonome Waffen nicht zu den Konzepten der bestehenden Waffensystemen passen und in ihrer Beispiellosigkeit über diese hinausgehen, dann gilt es neue Regulierungen und Debatten zur moralischen Zulässigkeit dieser neuen Waffensysteme zu erarbeiten.

(6) Sog. autonome Waffen fallen unter den Aspekt der Beispiellosigkeit, wenn sie mittels dem sog. *Generating Model* selbstständig die moralischen Dimensionen zur Wahl eines zulässigen Ziels nach der Definition des *ius in bello* berechnen und die Vernichtung des Ziels eigenständig durchführen. (*Game Changing Argument*)

(7) Ein Akteure handelt im philosophischen Sinne dann und nur dann autonom, wenn er aus selbst gewählten Gründen handelt.

(8) Maschinen und auch sog. autonome Waffen werden programmiert, gesteuert und oder von einem externen menschlichen Akteure überwacht.

(9) Maschinen und auch sog. autonome Waffen sind in jeder Weise vom Menschen fremdbestimmt und können daher nicht im philosophischen Sinne autonom handeln. Immer wohnen Absichten von anderen Handlungsträgen bei der Entscheidungsfindung einer Maschine bei. [6, 7, 8]

**(K) Sog. autonome Waffen sind nicht autonom. Somit fallen sie unter die rechtlichen sowie moralischen Regulierungen der bestehenden Waffensysteme. [3, 4, 5, 9]**

### Unabhängigkeit von Maschinen

(1) Maschinen und auch sog. autonome Waffen sind nicht autonom.

(2) Unabhängig handelt ein Akteure, wenn er sich auf selbst gewählte Weise ein ihm zugetragenes Ziel erreicht.

(3) Eine Maschine kann im Sinne des *Execution Model* ein ihm vorprogrammiertes Ziel auf eigenständige Weise erreichen indem es die Umgebungsparameter verarbeitet und ein strategisches Vorgehen zur besten Erreichung des Ziels berechnet.
 
**(K) Sog. autonome Waffen können unabhängig handeln. [1, 2, 3]**
